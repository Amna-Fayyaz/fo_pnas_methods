---
title: Bioinformatic Pipeline - "Hiding in Plain Sight - Genome-wide recombination
  and horizontal gene transfer drive clonal diversity in Fusarium oxysporum fsp ciceris"
author: "Fayyaz A, Robinson G, Chang PL, Bekele D, Yimer SM, Carrasquilla-Garcia N, Negash K, Anand K. Surendrarao, Eric J.B. von Wettberg, Robbertse B, Seid Ahmedi, Tesfaye K, Fikre Ab, Farmer AD and Cook DR"
#date: '2023-02-07'
output:
  #rmdformats::readthedown:
  #  toc_depth: 5
  #  self_contained: true
  #  thumbnails: false
  #  lightbox: true
  #  gallery: true
  #  highlight: tango
  html_document: default
  #pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(reticulate)
```

---

## Contents

### 1. [Overview](#Overview)
### 2. [Genome Assembly](#assembly)
### 3. [Gene Annotation and Analyses](#gene_annotation)
### 4. [Identification of SNP and Genetic Groups](#read_mapping)
### 5. [Population Analyses](#pop_gen)
### 6. [References](#refs)

---

## Help with Rmarkdown
### https://holtzy.github.io/Pimp-my-rmd/
### https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf

## <a id="Overview"></a>1. Overview

Plot overview pipeline

## <a id="assembly"></a>2. Genome Assembly

```{r, eval=F}
#####################################################################################
######################### Need input from Amna/Peter ################################
#####################################################################################
```

A. Quality check with FastQC
B. Low quality reads and adapters trimmed using Trimmomatic v36
C. Error correction using ALLPATHS-LG
D. Assembly using A5
E. Genome completeness assessed using BUSCO v3 (Ascomycota odb9 dataset)
```{bash, eval=F}
## parameters to use for runnig BUSCO
docker run -it --rm -v $(pwd):/home/working -w /home/working chrishah/busco-docker run_BUSCO.py --in ./Fusarium_oxysporum.fasta --out Fusairum-BUSCO -l ./sordariomyceta_odb9 --mode genome --sp fusarium_graminearum --c 20
```


## <a id="gene_annotation"></a>3. Gene Annotation and Analyses

```{r, eval=F}
#####################################################################################
######################### Need input from Amna/Peter ################################
#####################################################################################
```

A. Average nucleotide identity calculated with Pyani

B. Annotation
The annotation was done By using MAKER,RepeatModeler, iprscan, Blastp and AHRD
 i. RepeatModeler: We generated the library for custom repeats using RepeatModeler and used these custom library of repeats to feed into MAKER
 
```{bash, eval=False}
 $:/home/localhost/adf/sw/bin/singularity shell /erdos/adf/sw/singularityImages/repeatmodeler-1.0.11--pl5.22.0_0.img

./dfam-tetools.sh --singularity --container=dfam/tetools:latest --trf_prgm=/erdos/adf/sw/trf409.linux64 -- BuildDatabase -name fusariumdatabase -engine ncbi Fo-Et-0090.fasta
./dfam-tetools.sh --singularity --container=dfam/tetools:latest --trf_prgm=/erdos/adf/sw/trf409.linux64 -- RepeatModeler -database fusariumdatabase -LTRStruct -pa 20 >& repeats.out

##structural annotation using MAKER
i=`ls *.fasta`
for j in $i; do  maker -g $j 2>&1 | tee round1_$j.log ; done
mkdir snap
cd snap
maker2zff -n ../Fo-Et-0090.all.gff
fathom genome.ann genome.dna -categorize 1000
fathom uni.ann uni.dna -export 1000 -plus
forge export.ann export.dna
hmm-assembler.pl Fo-Et-0090 . > Fo-Et-0090.hmm
##used this hmm file to train next round of MAKER for all of the genome
for j in $i; do  maker -g $j 2>&1 | tee round1_$j.log ; done

##functional annotation using iprscan, blastp and AHRD for example using one genome "Fo-Et-0000"

handle_stop_codons.pl --stop_char '\*' Fo-Et-0000.all.maker.proteins.fasta > Fo-Et-0000.all.maker.proteins-stop-remove.fasta
mkdir chunks
fasta2chunks.pl --chunksize=1000 --chunk_prefix=chunks/chunk Fo-Et-0000.all.maker.proteins-stop-remove.fasta
IPRSCAN_HELPERS=/home/afayyaz/chado_preprocessing
JOB_LIMIT=32
export ANALYSIS_DATA_ROOT=`pwd`
mkdir iprscan; pushd iprscan
if [[ `which qsub` != "" ]]; then
for f in ../chunks/*; do echo ${IPRSCAN_HELPERS}/run_iprscan.bash $f | qsub -cwd -pe smp 8; done
else
for f in ../chunks/*; do
while (( `jobs | wc -l` >= $JOB_LIMIT )); do sleep 5; done;
${IPRSCAN_HELPERS}/run_iprscan.bash $f &
done
fi
#then to get iprscan "raw" output for AHRD
${IPRSCAN_HELPERS}/iprscan_convert.bash
mkdir ../iprscan_raw
mv *raw ../iprscan_raw
cd ../
mkdir ipr2go
for f in iprscan_raw/*; do ${IPRSCAN_HELPERS}/ipr2go.pl $f > `echo $f | sed 's/^iprscan_raw/ipr2go/'`; done
forkjobs_blastp_ahrd.pl --forks $JOB_LIMIT chunks blastp

perl -i ahrd_untruncate_blast_query_ids.pl blastp/fol/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/yeast/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/gram/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/vene/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/poae/*
mkdir AHRD
perl -p -e 's/\$\{([^}]+)\}/defined $ENV{$1} ? $ENV{$1} : $&/eg' /home/afayyaz/batcher_input_example_template.yml > AHRD/batcher_input_example.yml
ln -s ${INTERPRO_DB_ROOT}/interpro.dtd AHRD/interpro.dtd
pushd AHRD
mkdir batch_yml
java -cp ${AHRD_GIT_ROOT}/bin/ahrd.jar ahrd.controller.Batcher batcher_input_example.yml
perl -pi -e 's/$/ &/;' batcher.bash
source batcher.bash
for f in *csv; do if [[ ! -s all.ahrd.tsv ]]; then grep '^Protein-Accession' $f > all.ahrd.tsv; fi; sed -n '4,$p' $f >> all.ahrd.tsv; done
${IPRSCAN_HELPERS}/clean_AHRD.sh all.ahrd.tsv | sed 's/-mRNA-1//' > all.ahrd.tsv.cleaned
popd
cat iprscan_raw/* > all.iprscan_raw
awk 'BEGIN {FS="\t"} NF==9 && $2 == "maker" {print}' Fo-Et-0000.all.gff > Fo-Et-0000.all.maker.gff
##We added the annotation into gff files
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img ipr_update_gff Fo-Et-0000.all.maker.gff all.iprscan_raw > Fo-Et-0000.all.gff.iprscan_updated
${IPRSCAN_HELPERS}/add_note_attr_inGFF.pl AHRD/all.ahrd.tsv.cleaned Fo-Et-0000.all.gff.iprscan_updated > Fo-Et-0000.all.gff.iprscan_updated+AHRD
##We generated the gene id's and replace it with existing gene identifiers in the fasta and gff files.
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img maker_map_ids --prefix FO0000- Fo-Et-0000.all.gff > map
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_gff_ids map Fo-Et-0000.all.gff
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_fasta_ids map Fo-Et-0000.all.maker.proteins.fasta 
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_fasta_ids map Fo-Et-0000.all.maker.transcripts.fasta
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_gff_ids map Fo-Et-0000.all.gff.iprscan_updated+AHRD
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_data_ids map AHRD/all.ahrd.tsv.cleaned
cut -d'|' -f1,1 Fo-Et-0000.all.maker.proteins.fasta | sed 's!-RA!!g' >> Fo-Et-0000.all.maker.proteins.final.fasta
cut -d'|' -f1,1 Fo-Et-0000.all.maker.transcripts.fasta | sed 's!-RA!!g' >> Fo-Et-0000.all.maker.transcripts.final.fasta
/home/localhost/adf/ahrdify_fasta.pl AHRD/all.ahrd.tsv.cleaned Fo-Et-0000.all.maker.proteins.fasta Fo-Et-0000.all.maker.transcripts.fasta
##Repeated the same for rest of the genome
```
  
  ii. Gene Enrichment analysus was performed using BLAST2GO.
  iii. SIX genes homologs were identified using BLASTp with identity threshold >90% and coverage 90-100%
  iv. Orthologs were identified with Orthofinder(2.5.4)
```{bash, eval=False}
 singularity exec /home/afayyaz/singularity-images/orthofinder_2.5.4.sif orthofinder -t 50 -a 10 -f /home/afayyaz/99-isolate-orth-run/ > ortho_log_file 2>&1 
```
  
C. Phylogenetic Analyses
  Phylogenetic analysis used in the maunscript are performed using BEASTv1.10.4
```{bash, eval=False}
##Firstly, We extracted the single copy gene names/list form BUSCO results
perl busco_find_conserved_single_copy_across_all.pl run_*/full_table* > busco_find_conserved_single_copy_across_all.txt
##then we extracted the fasta sequences from the genoem files
for i in `cat busco_find_conserved_single_copy_across_all.txt`; do grep -A1 $i all-busco-single-copy-genes.fna --no-group-separator >> $i.fna; done
##We used clustalo for alignment of the 1556 genes
/home/afayyaz/bin/clustalo -i genes.fna -o genes-aligned.fasta --outfmt=fasta --threads 40
##then we used the aligned file, convert it into nexus file by using seqmagick(python package) and concatenate all of the nexus file into one file for beauti software 
python3.8 seqmagick.py convert --output-format nexus --alphabet dna /home/afayyaz/aligned-genes-fasta/EOG***.fna /home/afayyaz/edit-genes/EOG***.nex
python3.8 cat-nex.py
##generated the xml file in beauti and used in Beast 
java -jar /home/afayyaz/bin/BEASTv1.10.4/lib/beast.jar -threads 20 gene_alignment.xml

##used standard parameters for all of the phylogenetic trees and visulaized in iTOL
```
  
  - To check the congruence between different trees, we used Robinson-Foulds with ETE3 toolkit
```{bash, eval=False}

##We called SNP's on 1556 BUSCO single copy genes using snp-sites(https://github.com/sanger-pathogens/snp-sites)
for i in `ls *.fasta`; do snp-sites -rmv -o $i $i; done

##We removed all entries with missing data
cat *fasta.vcf > all_genes.fasta.vcf 
$awk '$0 !~ "_" {print}' all_genes.fasta.vcf > all_gene_snp_nomissing.vcf 
cat all_gene_snp_nomissing.vcf | awk -v OFS="\t" '$0 !~ "^#" {hom_REF = 0; hom_ALT = 0; ; hom_var_2 = 0; hom_var_3 = 0; het = 0; for(i=10;i<=NF;i++) { if($i ~ /0/) hom_REF++; else if($i ~ /1/) hom_ALT++; else if($i ~ /2/) hom_var_2++;else if($i ~ /3/) hom_var_3++; else het++; } print $1, $2,$3, $4, $5, hom_REF, hom_ALT, hom_var_2, hom_var_3, het}' >> maf-column.txt
paste all_gene_snp_nomissing.vcf maf-column.txt > all_gene_snp-maf-file.vcf

###fixed SNP's
for F in *.vcf.gz; do bgzip -dc $F | //
awk '{for(i=11;i<=NF;i++)if($i!=$(i-1)&&$1!~/#/)next}1' | //
bgzip -c > ${F}_filtered.vcf.gz; done 

##R code for selecting SNP's
library(dplyr)
file <- read.delim("all_gene_snp-maf-file.vcf.txt")
mydata <- data.frame(file)
data <- arrange(mydata, desc(maf))
subset1 <- subset(data, maf < 0.1 )
subset2 <- subset(data, maf >= 0.1 & maf < 0.2)
subset3 <- subset(data, maf >= 0.2 & maf < 0.3)
subset4 <- subset(data, maf >= 0.3 & maf < 0.4)
subset5 <- subset(data, maf > 0.4)
###Total number of SNP's are 706149
###No of SNP's in subset1 -> 299273
###No of SNP's in subset2 -> 402709
###No of SNP's in subset3 -> 2103
###No of SNP's in subset4 -> 1052
###No of SNP's in subset5 -> 1012
##We randomly selected snps from subsets based on the percentage representation in the total amount

x <- slice_sample(subset1, n=126831)
y <- slice_sample(subset2, n=229624) 
z <- slice_sample(subset3, n=6)
a <- slice_sample(subset4, n=1)
b <- slice_sample(subset5, n=1)

#randomly selecting snps from subsets based on the percentage representation in the total amount
write.table(x, file = "x-export-file.txt", sep = "\t" )
write.table(y, file = "y-export-file.txt", sep = "\t" )
write.table(z, file = "z-export-file.txt", sep = "\t" )
write.table(a, file = "a-export-file.txt", sep = "\t" )
write.table(b, file = "b-export-file.txt", sep = "\t" )
#concatente in bash and add header of the original vcf file.

#For fixed SNP's

###Total number of SNP's are 613071
###No of SNP's in subset1 -> 607962
###No of SNP's in subset2 -> 3486
###No of SNP's in subset3 -> 795
###No of SNP's in subset4 -> 455
###No of SNP's in subset5 -> 329

x <- slice_sample(subset1, n=601882)
y <- slice_sample(subset2, n=20) 
z <- slice_sample(subset3, n=1)
a <- slice_sample(subset4, n=1)
b <- slice_sample(subset5, n=1)
write.table(x, file = "x-export-file.txt", sep = "\t" )
write.table(y, file = "y-export-file.txt", sep = "\t" )
write.table(z, file = "z-export-file.txt", sep = "\t" )
write.table(a, file = "a-export-file.txt", sep = "\t" )
write.table(b, file = "b-export-file.txt", sep = "\t" )
#concatente in bash and add header of the original vcf file.

##converted vcf format to fasta format and used 
cat subset.vcf | perl vcf_to_fasta.pl > vcftofasta.fasta
##generated the xml file in beauti and used in Beast 
java -jar /home/afayyaz/bin/BEASTv1.10.4/lib/beast.jar -threads 20 gene_alignment.xml
##generated 10 different trees newick files labeld as "1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt"
ete3 compare -taboutput -t 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt -r 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt > result.txt
```
  
D. Pangenome
  Pangenome was constructed using PanOCT software with percentage identity threshold 95% 
```{bash, eval=Flase}
singularity exec /home/afayyaz/singularity-images/panoct_3.23--pl526_1.sif panoct.pl -t blast-results.txt -g final-e-attribute.txt -P 99-all-protein.pep -f list-e.txt -b /home/afayyaz/group-e-panoct/ -i 95 -L 20 -M Y -F 1.33 -N Y -H Y -V Y -S Y -C Y -B Y -c 0,5,25,50,75,90,95,98,100 > panoct-test 2>&1
```
  
Before we calculated the nucleotide diversity for pan-genome, we removed the paralogs from the gene presence absence file
 
```{bash, eval=Flase}
###Removed paralogs from panoct result file
Python3 script-paralogs.py
###To calculate nucleotide diversity for pan-genome, we needed the files of each cluster in the pangenome containing number of genomes from 5 to 99
awk -F"\t" '{print >> ($101".txt")}' without-paralogs.txt  
###Then we made a list of the cluster in the pangenome
for i in `ls cluster*`; do sed $'s/\t/\\\n/g' $i| sed '/^$/d' | sed '$d' | sed '$d' > $i.txt; done
###We extracted the cluster sequence from each genome
for i in `ls cluster*.txt`; do python3 extract_seq.py final-99-isolate-transcript.fasta $i $i.fasta; done
###We aligned the sequence using mafft
for i in `ls *.fasta`; do mafft --thread 20 $i > $i.aligned.fasta; done
###We called SNP's using python package snp-sites(https://github.com/sanger-pathogens/snp-sites)
for i in `ls *.fasta`; do snp-sites -rmv -o $i $i; done
for i in `ls *.vcf`; do sed 's/*/_/g' $i | awk '$0 !~ "_" {print}' | awk '{for(i=1;i<=NF;i++){if($i==0){$i="0/0"}}}{$1=$1} 1' OFS="\t" | awk '{for(i=10;i<=NF;i++){if($i==1){$i="1/1"}}}{$1=$1} 1' OFS="\t"| awk '{for(i=10;i<=NF;i++){if($i==2){$i="2/2"}}}{$1=$1} 1' OFS="\t" > $i-p.vcf; done
###We calculated nucleotide diversity using vcftools
for i in `ls *-p.vcf`; do vcftools --vcf $i --site-pi --out $i-nucl_diversity; done
for i in `ls *.pi`; do sed '1d' $i > $i.txt ; done
##We took the average of each file
ls *.txt > file_list
while read line; do awk '{sum+=$2} END {print sum/NR}' $line >> avg-pi-file; done<list-avg-pi.txt
mv file_new avg-values
cp *-aligned-files/avg* .
for i in `ls avg-*`; do awk '{print FILENAME (NF?"\t":"") $0}' $i > $i.txt; done
cat *.txt > all-cluster-pi.txt
###We calculated tajimas-d using vcftools
for i in `ls *-p.vcf`; do vcftools --vcf $i --TajimaD 100000000 --out $i-tajimad.txt; done
for i in `ls *.Tajima.D.txt`; do sed '1d' $i> $i.txt; done
##We took the average of each file
while read line; do awk '{sum+=$5} END {print sum/NR}' $line >> avg-tajima-file; done<list-avg-tajima.txt
mv file_new avg-values
cp *-aligned-files/avg* .
for i in `ls avg-*`; do awk '{print FILENAME (NF?"\t":"") $0}' $i > $i.txt; done
cat *.D.txt > tajima.d.txt
##repeat these steps for all clusters containing number of genomes from 5 to 99
```
  

## <a id="read_mapping"></a>4. Identification of SNP and genetic groups

### A. Read mapping and Variant calling

Tools required:

- BWA MEM v0.7.9a-r786
- SAMtools v1.3.1
- Vcftools
- Bcftools
- Picard 
- GATK v4.1

```{bash, eval=FALSE}
#create index for F. oxysporum reference genome (Fol4287 - GCF_000149955.1)
bwa index [-a bwtsw|is] reference.fasta index_prefix
samtools faidx reference.fasta
##created sequence dictionary
java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar CreateSequenceDictionary REFERENCE=reference.fasta OUTPUT=reference.dict
##lign reads and assign read group
for i in $(ls *.fastq.gz | cut -d '_' -f1,2,3| uniq); do bwa mem -t 60 -R "@RG\tID:${i}\tSM:${i}\tPL:illumina\tPU:Lane1\tLB:${i}" reference.fasta ${i}_R1_pair_001.fastq.gz ${i}_R2_pair_001.fastq.gz > ${i}.sam; done
##sorting sam files
for i in `ls *.sam`; do java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar SortSam I=$i O=$i.bam SORT_ORDER=coordinate; done
##mark duplicates
for i in `ls *.bam`; do java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar MarkDuplicates I=$i O=$i-edit.bam REMOVE_DUPLICATES=true METRICS_FILE=$i-metrics.txt; done
##sort bam file
for i in `ls *-edit.bam`; do java -jar /erdos/cook_lab/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar BuildBamIndex INPUT=$i ; done
##called variants
for i in `ls *-edit.bam`; do gatk HaplotypeCaller -R reference.fasta -I $i -ploidy 1 --native-pair-hmm-threads 40 -ERC GVCF -O $i-raw_gVCF.vcf --spark-runner LOCAL; done
for i in `ls *-raw_gVCF.vcf`; do gatk GenotypeGVCFs -O $i-raw.vcf -R reference.fasta --spark-runner LOCAL -V $i; done
for i in `ls *-raw.vcf`; do bgzip -c $i > $i.gz; done
for i in `ls *-raw.vcf.gz`; do tabix -p vcf $i ; done
bcftools merge *vcf.gz -o a-merged.vcf
##extract SNP's and stats
gatk SelectVariants -R reference.fasta -V a-merged.vcf -select-type SNP -O c-snps.vcf
bcftools stats c-snps.vcf > stats-snps.txt
vcftools --vcf c-snps.vcf --out c-filtered --minDP 10 --max-missing 0.95 --minQ 30 --recode --recode-INFO-all
```


### B. Filtering VCF file 

Tools required:

- bcftools/1.16

- vcftools/b240116

- mummer/3.23

For most population analyses, we exclusively used high quality genomes (based on BUSCO completeness > 99%, N50, number of scaffolds and genome size) - see "Genome Assembly". This provided a total list of 120 genomes, 99 of which were taxonomically within the FOSC (Fig. 2). This is reduced from the earlier VCF file, which contained 320 isolates (Supplemental Fig. 1)

Reducing the VCF file from 320 to 120, was done using bcftools (custom bash script).

### i. We split the VCF into separate single isolate VCF files

```{bash, eval=F}
######## ./splitting_vcfs_retaining.sh ###########

#!/usr/bin/env bash
for file in *.vcf.gz; do
  for sample in `bcftools query -l $file`; do
    bcftools view -Oz -s $sample -o ${file/.vcf*/.$sample.vcf.gz} $file
  done
done
```

### ii. We indexed all the single isolate VCF files

```{bash, eval=F}
for f in *vcf.gz; do tabix -f -p vcf $f; done
```

### iii. We merged isolates from list of high quality genomes (information obtained from assembly data)

```{bash, eval=F}
bcftools merge --file-list isolate_list > ethiopia_fo_structure.vcf
```

### iv. We removed heterozygous/ambiguous SNPs using sed

```{bash, eval=F}
sed -i 's#0/1|0/2|0/3|0/4|1/2|1/3|1/4|2/3|2/4|3/4#[.]/[.]#g' ethiopia_fo_structure.vcf
```

### v. We removed SNPs with high missingness and evaluated MAF filtering

To ensure that the subsequent VCF was high quality, especially for linkage disequilibrium, we adjusted filters to learn how different filters affect the subsequent linkage disequilibrium analyses. 

The main two parameters were missingness and minor allele frequency (MAF). MAF was of particular importance to us, as often researchers adjust MAF cutoff as >0.05 without regard for the effects on the population. 

```{bash, eval=F}
#VCF file for STRUCTURE groupings
#Note that we kept the MAF filter for the STRUCTURE groupings only because it is generally recommended.
#vcftools --vcf ethiopia_fo_structure.vcf --maf 0.05 --max-missing 0.9 --recode --stdout | bgzip -c > all_ethiopia_isolates.0_05.vcf.gz 

############## Add that we pruned the data using PLINK too ###############

#total number of SNPs = 74668

#VCF file for population genetic analyses
vcftools --vcf ethiopia_fo_structure.vcf --max-missing 0.9 --recode --stdout | bgzip -c > all_ethiopia_isolates.vcf.gz #total number of SNPs = 136772

#note that this was also conducted but there were only minor differences in the outcomes. The overall clonal groupings remained the same. The LD decay curve showed minor differences

#include plot /home/guy/Documents/Research/Cook/fusarium_pnas_final/old/Linkage/ld_total_maf.png
```

### vi. To ensure SNPs were core components of the genome, mapping was informed by additional whole genome alignments (MUMmer)

This was done on per chromosome basis (Fol4287 1-15)

```{bash, eval=F}
#Here is the example of chromosome 7 (Suppl. Fig. 2)
for f in Fo-Et-0*; do nucmer -maxmatch -c 100 -p ${f}_chr7 chr7.fna ${f}; done #outputs delta file

# Whole genome alignments were visualized and filtered in R
Rscript ./mummer_coverage.R

# This led to the removal of any SNPs mapped to chromosomes 3, 6, 14 and 15 as well as supercontig 27 (end of chromosome 1)
# Any reference contigs that mapped poorly to the Illumina genomes were removed running awk itieratively
# e.g. awk '($0 !~ "Supercontig_2.27"){print}' all_ethiopia_isolates.vcf.gz
```


### C. Identification of clonal groups using STRUCTURE

Tools required:

- structure-console/2.3.4

- structure harvester (https://taylor0.biology.ucla.edu/structureHarvester/)

- CLUMPP/1.1.2

Subdividing clonal lineages in fungal populations remains challenging. Traditionally, reasearchers have determined clonality based on identical MLGs. This approach fails on a whole genome scale due to the accumulation of erroneous mutations. We employed two different methods to subdivide the lineages: phylogenetics (see Gene Annotation and Analyses - BEAST) and K clustering (via STRUCTURE). 

### i. <a id="struct"></a>Random subsampling SNPs

To avoid SNP selection bias, the VCF file was randomly subsampled at a rate of ~10%. 
Note that this script does not provide exactly 10% of the total number of SNPs (Table 1 - Non-clone corrected = 136)

```{bash, eval=F}

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub1.vcf.gz

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub2.vcf.gz

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub3.vcf.gz

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub4.vcf.gz

#etc.
```

VCF subsample files were converted into genotype files using a manual bash script

```{bash, eval=F}
############ ./convertvcftogeno.sh ############
#!/usr/bin/env bash
file=$1
out=${file}.out
bgzip -dc ${file} | awk '$1 ~ "Super" || $1 ~ "#CHROM" {print}' | awk '$1 !~ "##" {print}' | sed 's#0/0#0#g;s#1/1#1#g;s#2/2#2#g;s#3/3#3#g;s#4/4#4#g;s#0/1#-9#g;s#0/2#-9#g;s#0/3#-9#g;s#0/4#-9#g;s#1/2#-9#g;s#1/3#-9#g;s#1/4#-9#g;s#2/3#-9#g;s#2/4#-9#g;s#3/4#-9#g;s#./.#-9#g' | cut -f 2,10- | awk '
{ 
    for (i=1; i<=NF; i++)  {
        a[NR,i] = $i
    }
}
NF>p { p = NF }
END {    
    for(j=1; j<=p; j++) {
        str=a[1,j]
        for(i=2; i<=NR; i++){
            str=str" "a[i,j];
        }
        print str
    }
}' > $out
```

### ii. STRUCTURE
  
Ran STRUCTURE on each subsampling 10 times: burnin = 10,000 and MCMC reps = 50,000. K vals = 2-20. Check out Mainparams and Extraparams for additional technical information.
  
```{bash, eval=F}
# e.g.
#!/bin/bash
module load structure-console/2.3.4
structure -K 10 -o structure_large10-1-2
```

### iii. STRUCTUREharvester

Using outputs of STRUCTURE, combine them using STRUCTURE_HARVESTER (online web server: http://taylor0.biology.ucla.edu/structureHarvester/) to produce .indfiles for CLUMPP.

### iv. CLUMPP

Ran CLUMPP on HPC cluster to collate 10 permutations of each subsample. Used LargeKGreedy algorithm.

```{bash, eval=F}
#e.g.
#!/bin/bash
module load CLUMPP/1.1.2
CLUMPP K2.paramfile -i K12.indfile -k 12 -c 133 -r 10 -m 3 -w 0 -s 2 -o K12.outfile -j K12.miscfile
```

### v. STRUCTURE plotted with R script

```{bash, eval=F}
######### ./structure_plotting_larger_collection #############
```

### Clonal groups were assigned based on Euclidean distances

```{bash, eval=F}
######### ./plotting_structure_network.R ##########
./plotting_structure_network.R --list "list of all .outfile paths" --label "list of isolate names (in order)" --title "title of graph (usually K val included)"
```


## <a id="pop_gen"></a>5. Population Analyses

### A. Geographic Analyses

### i. Plotting onto maps in R

To create plots of geographic sampling (both chickpea and Fungal sampling)

```{bash, eval=F}
######### ./geo_plot_ethiopia.R ###########
```

### ii. Mantel tests

The Mantel test is a measure of correlation between two distance matrices. Here we want to understand how genetic difference (both sequence and presence/absence variation) is correlated with geography.
  
```{r}
##We prepared the co-ordinates for all infected sample into data frame for mental test
library(ade4)
location = read.delim("location-for-distance-plot-99-isolates.txt")
rownames(location) <- location[, "Sample"]
location <- location[, -1]
x = as.matrix(location)
y <- distm(x, fun = distGeo)
physical_distance <- melt(y)

n <- max(table(physical_distance$Sample)) + 1  # +1,  so we have diagonal of 
res <- lapply(with(physical_distance, split(value, physical_distance$Sample)), function(x) c(rep(NA, n - length(x)), x))
res <- do.call("rbind", res)
res <- rbind(res, rep(NA, n))
res <- as.dist(t(res))
res
##mantel test for accessary genome usimg Jaccarad distance
m <- max(table(pan_distance$X)) + 1  # +1,  so we have diagonal of 
res2 <- lapply(with(pan_distance, split(value, pan_distance$X)), function(x) c(rep(NA, m - length(x)), x))
res2 <- do.call("rbind", res2)
res2 <- rbind(res2, rep(NA, m))
res2 <- as.dist(t(res2))
res2
mantel.rtest(res, res2, nrepet = 9999)
###manetl test for core using Conserved Ortholog genes were also performed the same.
```
  
  
### B. Linkage disequilibrium

We calculated linkage disequilibrium using two different methods:

1. Pearson's correlation coefficient (r2) [implemented in PLINK]
2. Index of association (rbarD) [implemented in PoppR]

### i. Linkage Disequilibrium via r2

Tools required:

- plink/1.90

- bcftools/1.16

#### Preparing datasets for "clonally-derived" vs "ancestrally-derived" SNPs (Figure 4 and Table 1)

"Clonally-derived" SNPs are variants that are variable within clonal subgroups. 

"Ancestrally-derived" SNPs are variants that are fixed within clonal subgroups 

To do this, we separated the final VCF file (See Filtering VCF) into the subgroups defined by BEAST (phylogenetics) and STRUCTURE (K clustering).

1. Split the VCf file into groups

```{bash, eval=F}
######## ./splitting_vcfs_retaining.sh ###########

#!/usr/bin/env bash
for file in *.vcf.gz; do
  for sample in `bcftools query -l $file`; do
    bcftools view -Oz -s $sample -o ${file/.vcf*/.$sample.vcf.gz} $file
  done
done
```

2. Combine into VCF files on a clade basis using BCFtools

```{bash, eval=F}
#e.g. This is an example for clonal subgroup E. Note that this was done for all groups E-U
#note that "clade_e.txt" is a text file containing the list of isolates contained in subgroup E
bcftools merge -l clade_e.txt > clade_e.vcf.gz
```

3. Filtering step for clonally-derived SNPs
```{bash, eval=F}
bgzip -dc  $group.vcf.gz | // #input file lineages
awk '($0 ~ "1/1" && $0 ~ "0/0") || ($0 ~ "1/1" && $0 ~ "2/2") || //
($0 ~ "2/2" && $0 ~ "0/0") || ($0 ~ "3/3" && $0 ~ "0/0") || //
($0 ~ "4/4" && $0 ~ "0/0") || ($0 ~ "3/3" && $0 ~ "1/1") || //
($0 ~ "4/4" && $0 ~ "1/1") || ($0 ~ "3/3" && $0 ~ "2/2") || //
($0 ~ "4/4" && $0 ~ "2/2") || ($0 ~ "3/3" && $0 ~ "4/4") || //
$1 ~ /#/ {print $0}' | bgzip -c > $output.vcf.gz #output SNPs variable across each clonal group
#note that these files were used to generate Figure 4C (decay plot for each clonal lineage)

bcftools merge -l $(ls clade*) > var_clones.vcf.gz
```

4. Filtering step for ancestrally-derived SNPs

```{bash, eval=F}
#filter across clonal subgroup
for F in *.vcf.gz; do bgzip -dc $F | //
awk '{for(i=11;i<=NF;i++)if($i!=$(i-1)&&$1!~/#/)next}1' | //
bgzip -c > ${F}_filtered.vcf.gz; done #output SNPs fixed across entire clonal group

#choose one sequence from the subgroup VCF files
for F in *_filtered.vcf.gz; do bgzip -dc $F | //
cut -f 1-11 | //
bgzip -c > ${F}_represent.vcf.gz; done

#merge representative isolates
bcftools merge -l $(ls *_represent.vcf.gz) > eth_clonecorrected.vcf.gz
```

#### Linkage disequilibrium - r2

1. Run intra-chromosomal PLINK LD

```{bash, eval=F}
#code for clonally-derived SNPs
plink --vcf var_clones.vcf.gz --allow-extra-chr --r2 //
--ld-window-r2 0 --ld-window 99999 --out var_clone

#code for ancestrally-derived SNPs
plink --vcf total_clone.vcf.gz --allow-extra-chr --r2 --ld-window 99999 //
--ld-window-r2 0 --out total_clone

#note that PLINK automatically retains only biallelic 

#calculating bp difference from PLINK LD output
for f in *.ld; do awk -F' ' '{print $1, $2, $5, $7, ($2-$5) }' $f | sed 's/-//g;s/ /\t/g' > diff_calculated/$f.diff; done

#filtering for bp difference <75000 (saves memory to permit R-based analysis)
for f in *diff; do awk '($5 < 75000) {print}' $f > rm_large_diffs/$f.75000.ld; done

#rest of plotting done in R
```

2. Run inter-chromosomal PLINK LD

Since inter-chromosomal SNPs can create unreasonably large file sizes (>100GB), we subset the SNP files before running

```{bash, eval=F}
#selects ~2.5% of the total SNPs (~2000-3000 SNPs)
#run on both var_clone.vcf.gz and eth_clonecorrected.vcf.gz
awk 'BEGIN {srand()} !/^$/ { if (rand() <= .025 || FNR==1) print $0}' $IN.vcf.gz > $OUT-filtered.vcf.gz

#calculating interchromosomal LD
plink --vcf var_clone.vcf.gz-filtered.vcf.gz --allow-extra-chr --inter-chr --r2 //
--ld-window-r2 0 --out var_clone_inter

#calculating interchromosomal LD
plink --vcf eth_clonecorrected.vcf.gz-filtered.vcf.gz --allow-extra-chr --inter-chr --r2 //
--ld-window-r2 0 --out total_clone_inter

#mean of each LD was calculated (background LD)
# file.ld = any inter-chromosomal PLINK LD output
count=0; total=0; for i in $( awk '{ print $7; }' file.ld );\
do total=$(echo $total+$i | bc ); \
((count++)); done; echo "scale=2; $total / $count" | bc
```

3. Plotting r2 decay curves and genome-wide r2

r2 decay plots

```{r, eval=F}
############# ld_decay.R ##################
```

r2 genome-wide plots

```{r, eval=F}
############## making_ld_windows.Rmd ###############
```

Files found in /media/guy/Guy HardDrive/vcf_individuals_largerCollection/final_ld/clades

### ii. Linkage disequilibrium - rbarD

To complement the Pearson's correlation coefficient (r2) LD data, we included the index of association (rbarD)(Agapow and Burt, 2001)

One issue that we experienced with this metric though is that it is very computationally intensive (at least in PoppR). As a result, most SNP sets had to be downsampled to a size of ~3000 SNPs

We used the same subsamples as for [STRUCTURE](#struct). As a result, the rbarD calculations were averaged over the 10 subsamples.

We used PoppR (Kamvar et al, 2014) to run rbarD. The VCF files were converted into geno files using vcfR (Knaus et al, 2017)

The generalized framework for calculating the index of association (rbarD) in R:

![Insert image of pipeline]


## C. Nucleotide Diversity (pi)

Tools required:

- vcftools/b240116

Nucleotide diversity (pi) was calculated from VCF files (both [read-mapped](#read_mapping) and [BUSCO](#gene_annotation)) using VCFtools (Danecek et al, 2011)

```{bash, eval=F}
##As shown above in Pangenome
```



D. Tajima's D
E. Fst (Weighted)
  - Core SNP Fst
  - Pangenome Fst
  - BUSCO Fst

## <a id="refs"></a>6. References