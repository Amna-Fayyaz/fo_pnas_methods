---
title: Bioinformatic Pipeline - "Hiding in Plain Sight - Genome-wide recombination
  and horizontal gene transfer drive clonal diversity in Fusarium oxysporum fsp ciceris"
author: "Fayyaz A, Robinson G, Chang PL, Bekele D, Yimer SM, Carrasquilla-Garcia N, Negash K, Anand K. Surendrarao, Eric J.B. von Wettberg, Robbertse B, Seid Ahmedi, Tesfaye K, Fikre Ab, Farmer AD and Cook DR"
#date: '2023-02-07'
output:
  #rmdformats::readthedown:
  #  toc_depth: 5
  #  self_contained: true
  #  thumbnails: false
  #  lightbox: true
  #  gallery: true
  #  highlight: tango
  html_document: default
  #pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(reticulate)
```

---

## Contents

### 1. [Overview](#Overview)
### 2. [Genome Assembly](#assembly)
### 3. [Gene Annotation and Analyses](#gene_annotation)
### 4. [Identification of SNP and Genetic Groups](#read_mapping)
### 5. [Population Analyses](#pop_gen)
### 6. [References](#refs)

---

## Help with Rmarkdown
### https://holtzy.github.io/Pimp-my-rmd/
### https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf

## <a id="Overview"></a>1. Overview

Plot overview pipeline

## <a id="assembly"></a>2. Genome Assembly

```{r, eval=F}
#####################################################################################
######################### Need input from Amna/Peter ################################
#####################################################################################
```

A. Quality check with FastQC
B. Low quality reads and adapters trimmed using Trimmomatic v36
C. Error correction using ALLPATHS-LG
D. Assembly using A5
E. Genome completeness assessed using BUSCO v3 (Ascomycota odb9 dataset)
```{bash, eval=False}
## parameters to use for runnig BUSCO
docker run -it --rm -v $(pwd):/home/working -w /home/working chrishah/busco-docker run_BUSCO.py --in ./Fusarium_oxysporum.fasta --out Fusairum-BUSCO -l ./sordariomyceta_odb9 --mode genome --sp fusarium_graminearum --c 20
```


## <a id="gene_annotation"></a>3. Gene Annotation and Analyses

```{r, eval=F}
#####################################################################################
######################### Need input from Amna/Peter ################################
#####################################################################################
```

A. Average nucleotide identity calculated with Pyani
```{bash, eval=False}
/home/localhost/bin/singularity run /home/localhost/singularityImages/average_nucleotide_identity-v0.2.9.simg -i all-file-pyani/ -g -m ANIb --workers 20 -o pyani-results
```
B. Annotation
 - RepeatModeler
 
```{bash, eval=False}
## to generate library for custom repeats using RepeatModeler
 $:/home/localhost/adf/sw/bin/singularity shell /erdos/adf/sw/singularityImages/repeatmodeler-1.0.11--pl5.22.0_0.img

./dfam-tetools.sh --singularity --container=dfam/tetools:latest --trf_prgm=/erdos/adf/sw/trf409.linux64 -- BuildDatabase -name fusariumdatabase -engine ncbi Fo-Et-0090.fasta
./dfam-tetools.sh --singularity --container=dfam/tetools:latest --trf_prgm=/erdos/adf/sw/trf409.linux64 -- RepeatModeler -database fusariumdatabase -LTRStruct -pa 20 >& repeats.out
##used custom library of repeats to feed into MAKER
```

  - MAKER
```{bash, eval=False}
##structural annotation
i=`ls *.fasta`
for j in $i; do  maker -g $j 2>&1 | tee round1_$j.log ; done
mkdir snap
cd snap
maker2zff -n ../Fo-Et-0090.all.gff
fathom genome.ann genome.dna -categorize 1000
fathom uni.ann uni.dna -export 1000 -plus
forge export.ann export.dna
hmm-assembler.pl Fo-Et-0090 . > Fo-Et-0090.hmm
##used this hmm file to train next round of MAKER for all of the genome
for j in $i; do  maker -g $j 2>&1 | tee round1_$j.log ; done

##functional annotation using iprscan, blastp and AHRD

handle_stop_codons.pl --stop_char '\*' Fo-Et-0000.all.maker.proteins.fasta > Fo-Et-0000.all.maker.proteins-stop-remove.fasta
mkdir chunks
fasta2chunks.pl --chunksize=1000 --chunk_prefix=chunks/chunk Fo-Et-0000.all.maker.proteins-stop-remove.fasta
IPRSCAN_HELPERS=/erdos/adf/chado_preprocessing
JOB_LIMIT=32
export ANALYSIS_DATA_ROOT=`pwd`
mkdir iprscan; pushd iprscan
if [[ `which qsub` != "" ]]; then
for f in ../chunks/*; do echo ${IPRSCAN_HELPERS}/run_iprscan.bash $f | qsub -cwd -pe smp 8; done
else
for f in ../chunks/*; do
while (( `jobs | wc -l` >= $JOB_LIMIT )); do sleep 5; done;
${IPRSCAN_HELPERS}/run_iprscan.bash $f &
done
fi
#then to get iprscan "raw" output for AHRD
${IPRSCAN_HELPERS}/iprscan_convert.bash
mkdir ../iprscan_raw
mv *raw ../iprscan_raw
cd ../
mkdir ipr2go
for f in iprscan_raw/*; do ${IPRSCAN_HELPERS}/ipr2go.pl $f > `echo $f | sed 's/^iprscan_raw/ipr2go/'`; done
forkjobs_blastp_ahrd.pl --forks $JOB_LIMIT chunks blastp

perl -i ahrd_untruncate_blast_query_ids.pl blastp/fol/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/yeast/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/gram/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/vene/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/poae/*
mkdir AHRD
perl -p -e 's/\$\{([^}]+)\}/defined $ENV{$1} ? $ENV{$1} : $&/eg' /home/afayyaz/batcher_input_example_template.yml > AHRD/batcher_input_example.yml
ln -s ${INTERPRO_DB_ROOT}/interpro.dtd AHRD/interpro.dtd
pushd AHRD
mkdir batch_yml
java -cp ${AHRD_GIT_ROOT}/bin/ahrd.jar ahrd.controller.Batcher batcher_input_example.yml
perl -pi -e 's/$/ &/;' batcher.bash
source batcher.bash
for f in *csv; do if [[ ! -s all.ahrd.tsv ]]; then grep '^Protein-Accession' $f > all.ahrd.tsv; fi; sed -n '4,$p' $f >> all.ahrd.tsv; done
${IPRSCAN_HELPERS}/clean_AHRD.sh all.ahrd.tsv | sed 's/-mRNA-1//' > all.ahrd.tsv.cleaned
popd
cat iprscan_raw/* > all.iprscan_raw
awk 'BEGIN {FS="\t"} NF==9 && $2 == "maker" {print}' Fo-Et-0000.all.gff > Fo-Et-0000.all.maker.gff
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img ipr_update_gff Fo-Et-0000.all.maker.gff all.iprscan_raw > Fo-Et-0000.all.gff.iprscan_updated
${IPRSCAN_HELPERS}/add_note_attr_inGFF.pl AHRD/all.ahrd.tsv.cleaned Fo-Et-0000.all.gff.iprscan_updated > Fo-Et-0000.all.gff.iprscan_updated+AHRD
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img maker_map_ids --prefix FO0000- Fo-Et-0000.all.gff > map
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_gff_ids map Fo-Et-0000.all.gff
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_fasta_ids map Fo-Et-0000.all.maker.proteins.fasta 
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_fasta_ids map Fo-Et-0000.all.maker.transcripts.fasta
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_gff_ids map Fo-Et-0000.all.gff.iprscan_updated+AHRD
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_data_ids map AHRD/all.ahrd.tsv.cleaned
cut -d'|' -f1,1 Fo-Et-0000.all.maker.proteins.fasta | sed 's!-RA!!g' >> Fo-Et-0000.all.maker.proteins.final.fasta
cut -d'|' -f1,1 Fo-Et-0000.all.maker.transcripts.fasta | sed 's!-RA!!g' >> Fo-Et-0000.all.maker.transcripts.final.fasta
/home/localhost/adf/ahrdify_fasta.pl AHRD/all.ahrd.tsv.cleaned Fo-Et-0000.all.maker.proteins.fasta Fo-Et-0000.all.maker.transcripts.fasta
##Repeated the same for rest of the genome
```
  
  - Gene Enrichment with BLAST2GO????
  - SIX genes homologs BLASTp????
  - Orthologs identified with Orthofinder (default excpt with -msa)
```{bash, eval=False}
 singularity exec /home/afayyaz/singularity-images/orthofinder_2.5.4.sif orthofinder -t 50 -a 10 -f /home/afayyaz/99-isolate-orth-run/ > ortho_log_file 2>&1 
```
  
C. Phylogenetic Analyses
  - BEAST
```{bash, eval=False}
##to extract the single copy gene names/list form BUSCO results
perl busco_find_conserved_single_copy_across_all.pl run_*/full_table* > busco_find_conserved_single_copy_across_all.txt
##to extract the fasta sequences from the 
for i in `cat busco_find_conserved_single_copy_across_all.txt`; do grep -A1 $i all-busco-single-copy-genes.fna --no-group-separator >> $i.fna; done
##used clustalo for alignment of the 1556 genes
/home/afayyaz/bin/clustalo -i genes.fna -o genes-aligned.fasta --outfmt=fasta --threads 40
##used the aligned file, convert it into nexus file and concatenate all of the nexus file into one file for beauti software 
python3.8 seqmagick.py convert --output-format nexus --alphabet dna /home/afayyaz/aligned-genes-fasta/EOG***.fna /home/afayyaz/edit-genes/EOG***.nex
python3.8 cat-nex.py
##generated the xml file in beauti and used in Beast 
java -jar /home/afayyaz/bin/BEASTv1.10.4/lib/beast.jar -threads 20 gene_alignment.xml

##used same parameters for all of the phylogenetic trees and visulaized in iTOL
```
  
  - Robinson-Foulds with ETE3 toolkit
```{bash, eval=False}

##calling SNP's on 1556 BUSCO single copy genes
for i in `ls *.fasta`; do snp-sites -rmv -o $i $i; done

#removed all entries with missing data
cat *fasta.vcf > all_genes.fasta.vcf 
$awk '$0 !~ "_" {print}' all_genes.fasta.vcf > all_gene_snp_nomissing.vcf 
cat all_gene_snp_nomissing.vcf | awk -v OFS="\t" '$0 !~ "^#" {hom_REF = 0; hom_ALT = 0; ; hom_var_2 = 0; hom_var_3 = 0; het = 0; for(i=10;i<=NF;i++) { if($i ~ /0/) hom_REF++; else if($i ~ /1/) hom_ALT++; else if($i ~ /2/) hom_var_2++;else if($i ~ /3/) hom_var_3++; else het++; } print $1, $2,$3, $4, $5, hom_REF, hom_ALT, hom_var_2, hom_var_3, het}' >> maf-column.txt
paste all_gene_snp_nomissing.vcf maf-column.txt > all_gene_snp-maf-file.vcf

###fixed SNP's
????????????????????

##R code for selecting SNP's
library(dplyr)
file <- read.delim("all_gene_snp-maf-file.vcf.txt")
mydata <- data.frame(file)
data <- arrange(mydata, desc(maf))
subset1 <- subset(data, maf < 0.1 )
subset2 <- subset(data, maf >= 0.1 & maf < 0.2)
subset3 <- subset(data, maf >= 0.2 & maf < 0.3)
subset4 <- subset(data, maf >= 0.3 & maf < 0.4)
subset5 <- subset(data, maf > 0.4)
###Total number of SNP's are 706149
###No of SNP's in subset1 -> 299273
###No of SNP's in subset2 -> 402709
###No of SNP's in subset3 -> 2103
###No of SNP's in subset4 -> 1052
###No of SNP's in subset5 -> 1012
#randomly selecting snps from subsets based on the percentage representation in the total amount

x <- slice_sample(subset1, n=126831)
y <- slice_sample(subset2, n=229624) 
z <- slice_sample(subset3, n=6)
a <- slice_sample(subset4, n=1)
b <- slice_sample(subset5, n=1)

#randomly selecting snps from subsets based on the percentage representation in the total amount
write.table(x, file = "x-export-file.txt", sep = "\t" )
write.table(y, file = "y-export-file.txt", sep = "\t" )
write.table(z, file = "z-export-file.txt", sep = "\t" )
write.table(a, file = "a-export-file.txt", sep = "\t" )
write.table(b, file = "b-export-file.txt", sep = "\t" )
#concatente in bash and add header of the original vcf file.

#For fixed SNP's

#randomly selecting snps from subsets based on the percentage representation in the total amount
file <- read.delim("all_gene_snp-maf-file.vcf.txt")
mydata <- data.frame(file)
data <- arrange(mydata, desc(maf))
subset1 <- subset(data, maf < 0.1 )
subset2 <- subset(data, maf >= 0.1 & maf < 0.2)
subset3 <- subset(data, maf >= 0.2 & maf < 0.3)
subset4 <- subset(data, maf >= 0.3 & maf < 0.4)
subset5 <- subset(data, maf > 0.4)

###Total number of SNP's are 613071
###No of SNP's in subset1 -> 607962
###No of SNP's in subset2 -> 3486
###No of SNP's in subset3 -> 795
###No of SNP's in subset4 -> 455
###No of SNP's in subset5 -> 329

x <- slice_sample(subset1, n=601882)
y <- slice_sample(subset2, n=20) 
z <- slice_sample(subset3, n=1)
a <- slice_sample(subset4, n=1)
b <- slice_sample(subset5, n=1)
write.table(x, file = "x-export-file.txt", sep = "\t" )
write.table(y, file = "y-export-file.txt", sep = "\t" )
write.table(z, file = "z-export-file.txt", sep = "\t" )
write.table(a, file = "a-export-file.txt", sep = "\t" )
write.table(b, file = "b-export-file.txt", sep = "\t" )
#concatente in bash and add header of the original vcf file.

##converted vcf formate to fasta format and used 
cat subset_guy.vcf | perl vcf_to_fasta.pl > vcftofasta.fasta
##generated the xml file in beauti and used in Beast 
java -jar /home/afayyaz/bin/BEASTv1.10.4/lib/beast.jar -threads 20 gene_alignment.xml
##generated 10 different trees newick files labeld as "1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt"
ete3 compare -taboutput -t 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt -r 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt > result.txt
```
  
D. Pangenome

  - PanOCT
```{bash, eval=Flase}
###Remove paralogs from panoct result file
Python3 script-paralogs.py
###To make the files of each cluster in the pangenome containing number of genomes from 5 to 99
awk -F"\t" '{print >> ($101".txt")}' without-paralogs.txt  
###to make a list of the cluster in the pangenome
for i in `ls cluster*`; do sed $'s/\t/\\\n/g' $i| sed '/^$/d' | sed '$d' | sed '$d' > $i.txt; done
###To convert sequence into single line fasta
awk '/^>/ { print (NR==1 ? "" : RS) $0; next } { printf "%s", $0 } END { printf RS }' 99-transcript.fasta > final-99-isolate-transcript.fasta
###To grep the sequence from file, aligned the sequence, and called SNP's
for i in `ls cluster*.txt`; do python3 extract_seq.py final-99-isolate-transcript.fasta $i $i.fasta; done
for i in `ls *.fasta`; do mafft --thread 20 $i > $i.aligned.fasta; done
for i in `ls *.fasta`; do snp-sites -rmv -o $i $i; done
for i in `ls *.vcf`; do sed 's/*/_/g' $i | awk '$0 !~ "_" {print}' | awk '{for(i=1;i<=NF;i++){if($i==0){$i="0/0"}}}{$1=$1} 1' OFS="\t" | awk '{for(i=10;i<=NF;i++){if($i==1){$i="1/1"}}}{$1=$1} 1' OFS="\t"| awk '{for(i=10;i<=NF;i++){if($i==2){$i="2/2"}}}{$1=$1} 1' OFS="\t" > $i-p.vcf; done
###to calculate nucleotide diversity and tajimas-d
for i in `ls *-p.vcf`; do vcftools --vcf $i --site-pi --out $i-nucl_diversity; done
for i in `ls *.pi`; do sed '1d' $i > $i.txt ; done
ls *.txt > file_list
while read line; do awk '{sum+=$3} END {print sum/NR}' $line >> file_new; done<file_list
mv file_new avg-values-
cp *-aligned-files/avg* .
for i in `ls avg-*`; do awk '{print FILENAME (NF?"\t":"") $0}' $i > $i-for-r; done
cat *for-r > all-cluster-pi.txt
ls *-for-r > list-avg-avg
while read line; do awk '{sum+=$2} END {print sum/NR}' $line >> avg-pi-file; done<list-avg-pi.txt
while read line; do awk '{sum+=$5} END {print sum/NR}' $line >> avg-tajima-file; done<list-avg-tajima.txt
for i in `ls *-p.vcf`; do vcftools --vcf $i --TajimaD 100000000 --out $i-tajimad.txt; done
for i in `ls *.Tajima.D.txt`; do sed '1d' $i> $i.txt; done
cat *.D.txt > tajima.d.txt
##repeat these steps for all clusters containing number of genomes from 5 to 99

```
  

## <a id="read_mapping"></a>4. Identification of SNP and genetic groups

### A. Read mapping and Variant calling

Tools required:

- BWA MEM v0.7.9a-r786
- SAMtools v1.3.1
- Vcftools
- Bcftools
- Picard 
- GATK v4.1

```{bash, eval=FALSE}
#create index for F. oxysporum reference genome (Fol4287 - GCF_000149955.1)
bwa index [-a bwtsw|is] reference.fasta index_prefix
samtools faidx reference.fasta
##created sequence dictionary
java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar CreateSequenceDictionary REFERENCE=reference.fasta OUTPUT=reference.dict
##lign reads and assign read group
for i in $(ls *.fastq.gz | cut -d '_' -f1,2,3| uniq); do bwa mem -t 60 -R "@RG\tID:${i}\tSM:${i}\tPL:illumina\tPU:Lane1\tLB:${i}" reference.fasta ${i}_R1_pair_001.fastq.gz ${i}_R2_pair_001.fastq.gz > ${i}.sam; done
##sorting sam files
for i in `ls *.sam`; do java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar SortSam I=$i O=$i.bam SORT_ORDER=coordinate; done
##mark duplicates
for i in `ls *.bam`; do java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar MarkDuplicates I=$i O=$i-edit.bam REMOVE_DUPLICATES=true METRICS_FILE=$i-metrics.txt; done
##sort bam file
for i in `ls *-edit.bam`; do java -jar /erdos/cook_lab/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar BuildBamIndex INPUT=$i ; done
##called variants
for i in `ls *-edit.bam`; do gatk HaplotypeCaller -R reference.fasta -I $i -ploidy 1 --native-pair-hmm-threads 40 -ERC GVCF -O $i-raw_gVCF.vcf --spark-runner LOCAL; done
for i in `ls *-raw_gVCF.vcf`; do gatk GenotypeGVCFs -O $i-raw.vcf -R reference.fasta --spark-runner LOCAL -V $i; done
for i in `ls *-raw.vcf`; do bgzip -c $i > $i.gz; done
for i in `ls *-raw.vcf.gz`; do tabix -p vcf $i ; done
bcftools merge *vcf.gz -o a-merged.vcf
##extract SNP's and stats
gatk SelectVariants -R reference.fasta -V a-merged.vcf -select-type SNP -O c-snps.vcf
bcftools stats c-snps.vcf > stats-snps.txt
vcftools --vcf c-snps.vcf --out c-filtered --minDP 10 --max-missing 0.95 --minQ 30 --recode --recode-INFO-all
```


### B. Filtering VCF file 

Tools required:

bcftools/1.16

vcftools/b240116

For most population analyses, we exclusively used high quality genomes (based on BUSCO completeness > 97%, N50, number of scaffolds and genome size) - see "Genome Assembly". This provided a total list of 120 genomes, 99 of which were taxonomically within the FOSC (Fig. 2). This is reduced from the earlier VCF file, which contained 320 isolates (Supplemental Fig. 1)

Reducing the VCF file from 320 to 120, was done using bcftools (custom bash script).

First, we split the VCF into separate single isolate VCF files

```{bash, eval=F}
######## ./splitting_vcfs_retaining.sh ###########

#!/usr/bin/env bash
for file in *.vcf.gz; do
  for sample in `bcftools query -l $file`; do
    bcftools view -Oz -s $sample -o ${file/.vcf*/.$sample.vcf.gz} $file
  done
done
```

Second, we indexed all the single isolate VCF files

```{bash, eval=F}
for f in *vcf.gz; do tabix -f -p vcf $f; done
```

Third, we merged isolates from list of high quality genomes (information obtained from assembly data)

```{bash, eval=F}
bcftools merge --file-list isolate_list > ethiopia_fo_structure.vcf
```

To ensure that the subsequent VCF was high quality, especially for linkage disequilibrium, we adjusted filters to learn how different filters affect the subsequent linkage disequilibrium analyses. 

The main two parameters were missingness and minor allele frequency (MAF). MAF was of particular importance to us, as often researchers adjust MAF cutoff as >0.05 without regard for the effects on the population. 

```{bash, eval=F}
#VCF 
vcftools --vcf ethiopia_fo_structure.vcf --maf 0.05 --max-missing 0.9 --recode --stdout | bgzip -c > all_ethiopia_isolates.0_05.vcf.gz

#note that this was also conducted but there were only minor differences in the outcomes. The overall clonal groupings remained the same. The LD decay curve showed minor differences

#include plot /home/guy/Documents/Research/Cook/fusarium_pnas_final/old/Linkage/ld_total_maf.png
```


To do this we 
  - missingness
  - maf
  - heterozygous
  - masking ambiguous SNP calls
  - Mapping informed with MUMmer whole genome alignments
  
C. Identification of clonal groups
  - Subsampling SNPs ~10% X10 times
  - STRUCTURE
  - STRUCTUREharvester - Evanno stats
  - CLUMPP v1.1.2 merge all 10 replicates (LargeKgreedy)
  - STRUCTURE plotted with R script
  - Assigning clonal groupings using Euclidean distances

## <a id="pop_gen"></a>5. Population Analyses

A. Geographic Analyses
  - Plotting onto maps in R
  - Mantel tests
B. Linkage disequilibrium
  - Additional filtering for derived SNPs and ancestral SNPs
    - BASH scripts
  - LD with r2 (PLINK)
    - Inter and intra-chromosomal LD
    - Genome-wide LD across genome
  - LD with rbarD (Poppr)
C. Nucleotide Diversity (pi)
D. Tajima's D
E. Fst (Weighted)
  - Core SNP Fst
  - Pangenome Fst
  - BUSCO Fst

## <a id="refs"></a>6. References